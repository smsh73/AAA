# 자료수집 기능 개선 완료 요약

## 구현 완료된 개선 사항

### 1. 작업 완료 자동 감지 로직 ✅

#### 구현 내용
- `_check_job_completion()` 함수 추가: 모든 수집 타입의 작업 완료 여부를 자동으로 확인
- `check_job_completion_task` Celery 태스크 추가: 주기적으로 작업 완료 상태를 확인
- 각 수집 작업 완료 시 자동으로 완료 여부 체크 및 상태 업데이트

#### 동작 방식
1. 각 `collect_data_task` 완료 시 `_check_job_completion()` 호출
2. 모든 타입의 작업이 완료되면 `CollectionJob.status`를 "completed"로 변경
3. `start_collection_job_task` 실행 후 5분 뒤에 `check_job_completion_task` 자동 실행
4. 작업이 아직 진행 중이면 1분마다 재확인

### 2. 에러 처리 강화 ✅

#### 구현 내용
- `collect_data_task`에서 에러 발생 시에도 CollectionJob 진행 상황 업데이트
- DB 업데이트 실패 시에도 원래 에러는 전파하도록 처리
- `start_collection_job_task`에서 예외 발생 시 작업 상태를 "failed"로 변경

#### 개선 사항
- 중첩된 try-except 블록으로 에러 처리 격리
- DB 업데이트 실패가 원래 작업 실패를 가리지 않도록 처리

### 3. 타임아웃 처리 개선 ✅

#### 구현 내용
- `PerplexityService.search()` 메서드에 타임아웃 처리 추가
- `httpx.Timeout(300.0, connect=10.0)` 설정: 전체 요청 300초, 연결 10초
- 타임아웃, HTTP 오류, 연결 오류를 구분하여 예외 처리

#### 예외 타입
- `TimeoutError`: 요청 시간 초과
- `ValueError`: HTTP 상태 코드 오류
- `ConnectionError`: 연결 오류

### 4. 진행 상황 추적 개선 ✅

#### 구현 내용
- 각 수집 작업 완료 시 전체 진행률 자동 계산 및 업데이트
- `overall_progress` 필드를 실시간으로 업데이트
- 완료된 작업 수와 총 작업 수를 기반으로 진행률 계산

#### 계산 방식
```python
total_tasks = sum(p.get("total", 0) for p in job.progress.values())
completed_tasks = sum(p.get("completed", 0) for p in job.progress.values())
overall_progress = (completed_tasks / total_tasks * 100) if total_tasks > 0 else 0.0
```

## 마이그레이션 파일 생성

### 생성된 파일
- `apps/api/alembic/versions/002_add_collection_job.py`

### 마이그레이션 내용
1. **collection_jobs 테이블 생성**
   - 모든 필수 컬럼 및 관계 설정
   - 인덱스 생성 (analyst_id, status)

2. **data_collection_logs 테이블 수정**
   - `collection_job_id` 컬럼 추가
   - 외래 키 제약 조건 추가
   - 인덱스 생성

## 마이그레이션 실행 방법

### 방법 1: Alembic 사용 (권장)

데이터베이스 연결 정보를 설정한 후:

```bash
cd apps/api

# 환경 변수 설정 (필요한 경우)
export DATABASE_URL="postgresql://username:password@localhost:5432/analyst_awards"

# 마이그레이션 실행
alembic upgrade head
```

### 방법 2: 수동 SQL 실행

데이터베이스에 직접 연결하여 `002_add_collection_job.py`의 `upgrade()` 함수 내용을 SQL로 실행:

```sql
-- collection_jobs 테이블 생성
CREATE TABLE collection_jobs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    analyst_id UUID NOT NULL REFERENCES analysts(id),
    collection_types TEXT[] NOT NULL,
    start_date TIMESTAMP WITH TIME ZONE NOT NULL,
    end_date TIMESTAMP WITH TIME ZONE NOT NULL,
    status VARCHAR(20) NOT NULL DEFAULT 'pending',
    progress JSONB DEFAULT '{}',
    overall_progress VARCHAR(10) DEFAULT '0.0',
    estimated_completion_time TIMESTAMP WITH TIME ZONE,
    started_at TIMESTAMP WITH TIME ZONE,
    completed_at TIMESTAMP WITH TIME ZONE,
    error_message VARCHAR(500),
    created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_collection_jobs_analyst_id ON collection_jobs(analyst_id);
CREATE INDEX idx_collection_jobs_status ON collection_jobs(status);

-- data_collection_logs 테이블 수정
ALTER TABLE data_collection_logs 
ADD COLUMN collection_job_id UUID REFERENCES collection_jobs(id);

CREATE INDEX idx_data_collection_logs_collection_job_id ON data_collection_logs(collection_job_id);
```

### 방법 3: init_db.py 사용 (개발 환경만)

**주의**: 이 방법은 기존 데이터를 삭제할 수 있습니다.

```bash
cd apps/api
python scripts/init_db.py
```

## 마이그레이션 확인

마이그레이션 실행 후 다음 쿼리로 확인:

```sql
-- 테이블 생성 확인
SELECT table_name FROM information_schema.tables 
WHERE table_schema = 'public' AND table_name = 'collection_jobs';

-- 컬럼 추가 확인
SELECT column_name FROM information_schema.columns 
WHERE table_name = 'data_collection_logs' AND column_name = 'collection_job_id';

-- 인덱스 확인
SELECT indexname FROM pg_indexes 
WHERE tablename IN ('collection_jobs', 'data_collection_logs');
```

## 테스트 방법

### 1. 작업 완료 자동 감지 테스트

```python
# API를 통해 수집 작업 시작
POST /api/data-collection/start
{
    "analyst_id": "...",
    "collection_types": ["target_price", "performance"],
    "start_date": "2024-01-01",
    "end_date": "2024-12-31"
}

# 작업 상태 확인 (자동으로 완료 상태로 변경되는지 확인)
GET /api/data-collection/{collection_job_id}/status
```

### 2. 에러 처리 테스트

- 잘못된 analyst_id로 작업 시작 시 에러 처리 확인
- Perplexity API 오류 시 로그 저장 및 작업 상태 업데이트 확인

### 3. 타임아웃 처리 테스트

- 네트워크 연결 끊김 시나리오 테스트
- Perplexity API 응답 지연 시나리오 테스트

### 4. 진행 상황 추적 테스트

- 작업 시작 후 진행률이 실시간으로 업데이트되는지 확인
- 완료 시 진행률이 100%로 변경되는지 확인

## 다음 단계

1. ✅ 마이그레이션 실행
2. ✅ 통합 테스트 수행
3. ✅ 프로덕션 환경 배포 전 검증

## 주의사항

1. **프로덕션 환경**: 마이그레이션 전 반드시 데이터베이스 백업 수행
2. **기존 데이터**: 기존 `data_collection_logs` 레코드의 `collection_job_id`는 NULL이 될 수 있음
3. **성능**: 대량의 데이터가 있는 경우 인덱스 생성에 시간이 걸릴 수 있음

