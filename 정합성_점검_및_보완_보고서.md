# 자료수집 및 KPI 평가 정합성 점검 및 보완 보고서

## 1. 점검 범위

### 1.1 점검 항목
- 사용자 화면 → 기능 → 소스코드 → API 호출 → 스키마 정합성
- 자료수집 기능 전체 흐름
- KPI 평가 기능 전체 흐름

### 1.2 점검 방법
- 프론트엔드 화면 확인
- API 엔드포인트 확인
- 서비스 로직 확인
- 스키마와 모델 일치성 확인
- 데이터 흐름 추적

## 2. 자료수집 기능 정합성 점검

### 2.1 화면 → API 호출

**프론트엔드**: `apps/web/src/app/data-collection/page.tsx`
- ✅ `/api/data-collection/start` (POST) - 데이터 수집 시작
- ✅ `/api/data-collection/{job_id}/status` (GET) - 작업 상태 조회
- ✅ `/api/data-collection/logs` (GET) - 수집 로그 조회

**백엔드**: `apps/api/app/routers/data_collection.py`
- ✅ `POST /api/data-collection/start` - 구현됨
- ✅ `GET /api/data-collection/{collection_job_id}/status` - 구현됨
- ✅ `GET /api/data-collection/logs` - 구현됨
- ✅ `GET /api/data-collection/analysts/{analyst_id}/logs` - 구현됨

### 2.2 API → 스키마

**스키마**: `apps/api/app/schemas/data_collection.py`
- ✅ `DataCollectionStartRequest` - `analyst_id`, `collection_types`, `start_date`, `end_date`
- ✅ `DataCollectionStartResponse` - `collection_job_id`, `status`, `estimated_completion_time`
- ✅ `DataCollectionStatusResponse` - 모든 필드 일치
- ✅ `DataCollectionLogResponse` - 모델과 일치

### 2.3 스키마 → 모델

**모델**: `apps/api/app/models/data_collection_log.py`, `collection_job.py`
- ✅ `DataCollectionLog` 모델과 `DataCollectionLogResponse` 스키마 일치
- ✅ `CollectionJob` 모델과 `DataCollectionStatusResponse` 스키마 일치

### 2.4 발견된 문제점

#### 문제 1: DataCollectionLog 스키마에 누락된 필드
- 모델에는 있지만 스키마에 없는 필드:
  - `prompt_template_id` (모델에 있음, 스키마에 없음)
  - `perplexity_request` (모델에 있음, 스키마에 없음)
  - `perplexity_response` (모델에 있음, 스키마에 없음)
  - `token_usage` (모델에 있음, 스키마에 없음)

**영향**: 로그 상세 정보가 API 응답에 포함되지 않음

## 3. KPI 평가 기능 정합성 점검

### 3.1 화면 → API 호출

**프론트엔드**: 
- `apps/web/src/app/evaluations/[id]/page.tsx` - 평가 상세
  - ✅ `/api/evaluations/{evaluation_id}` (GET) - 평가 조회
  - ✅ `/api/evaluations/{evaluation_id}/scores` (GET) - 점수 조회
- `apps/web/src/app/scorecards/[id]/page.tsx` - 스코어카드 상세
  - ✅ `/api/scorecards/{scorecard_id}` (GET) - 스코어카드 조회

**백엔드**: `apps/api/app/routers/evaluations.py`, `scorecards.py`
- ✅ `GET /api/evaluations/{evaluation_id}` - 구현됨
- ✅ `GET /api/evaluations/{evaluation_id}/scores` - 구현됨
- ✅ `GET /api/scorecards/{scorecard_id}` - 구현됨

### 3.2 API → 스키마

**스키마**: `apps/api/app/schemas/evaluation.py`, `scorecard.py`
- ✅ `EvaluationDetailResponse` - 모든 필드 일치
- ✅ `EvaluationScoreResponse` - 모든 필드 일치
- ✅ `ScorecardDetailResponse` - 모든 필드 일치

### 3.3 스키마 → 모델

**모델**: `apps/api/app/models/evaluation.py`, `scorecard.py`
- ✅ `Evaluation` 모델과 `EvaluationDetailResponse` 스키마 일치
- ✅ `EvaluationScore` 모델과 `EvaluationScoreResponse` 스키마 일치
- ✅ `Scorecard` 모델과 `ScorecardDetailResponse` 스키마 일치

### 3.4 발견된 문제점

#### 문제 2: SNS 주목도 및 미디어 언급 빈도 하드코딩
- 위치: `apps/api/app/services/ai_agents/evaluation_agent.py`
- 문제: `_calculate_sns_attention_score()`와 `_calculate_media_frequency_score()`가 항상 50.0 반환
- 영향: 실제 SNS/미디어 데이터를 반영하지 못함

#### 문제 3: 스코어카드 생성 시 KPI 점수 누락
- 위치: `apps/api/app/services/evaluation_service.py` - `complete_evaluation()`
- 문제: 스코어카드 생성 시 `scores`에 KPI 점수(7개)가 아닌 상위 3개 점수만 포함
- 현재: `ai_quantitative_score`, `sns_market_score`, `expert_survey_score`만 포함
- 필요: 7개 KPI 점수 모두 포함 필요
  - `target_price_accuracy`
  - `performance_accuracy`
  - `investment_logic_validity`
  - `risk_analysis_appropriateness`
  - `report_frequency`
  - `sns_attention`
  - `media_frequency`

#### 문제 4: 평가 완료 후 스코어카드 자동 생성 누락
- 위치: `apps/api/app/services/ai_agents/evaluation_agent.py` - `evaluate_async()`
- 문제: `evaluate_async()` 완료 후 `complete_evaluation()` 호출이 없음
- 영향: 평가 완료 후 스코어카드가 자동으로 생성되지 않음

#### 문제 5: 평가 태스크 미구현
- 위치: `apps/api/app/tasks/evaluation_tasks.py`
- 문제: `evaluate_report_task`가 실제로 평가를 실행하지 않음 (주석 처리됨)
- 영향: Celery를 통한 비동기 평가가 작동하지 않음

## 4. 보완 작업

### 4.1 DataCollectionLog 스키마 보완

**파일**: `apps/api/app/schemas/data_collection.py`

추가할 필드:
- `prompt_template_id`: Optional[str]
- `perplexity_request`: Optional[Dict[str, Any]]
- `perplexity_response`: Optional[Dict[str, Any]]
- `token_usage`: Optional[Dict[str, Any]]

### 4.2 SNS 주목도 및 미디어 언급 빈도 실제 데이터 수집

**파일**: `apps/api/app/services/ai_agents/evaluation_agent.py`

수정 방법:
- `_calculate_sns_attention_score()`: DataCollectionLog에서 sns 타입 로그 조회하여 점수 계산
- `_calculate_media_frequency_score()`: DataCollectionLog에서 media 타입 로그 조회하여 점수 계산

### 4.3 스코어카드 생성 시 KPI 점수 포함

**파일**: `apps/api/app/services/evaluation_service.py`

수정 방법:
- `complete_evaluation()`에서 `EvaluationScore`를 조회하여 7개 KPI 점수를 모두 포함

### 4.4 평가 완료 후 스코어카드 자동 생성

**파일**: `apps/api/app/services/ai_agents/evaluation_agent.py`

수정 방법:
- `evaluate_async()` 완료 후 `EvaluationService.complete_evaluation()` 호출 추가

### 4.5 평가 태스크 구현

**파일**: `apps/api/app/tasks/evaluation_tasks.py`

수정 방법:
- `evaluate_report_task`에서 실제로 `evaluate_async()` 실행
- 완료 후 `complete_evaluation()` 호출

## 5. 정합성 점검 결과 요약

### 5.1 자료수집 기능
- ✅ 화면 → API 호출: 정상
- ✅ API → 스키마: 대부분 정상 (일부 필드 누락)
- ⚠️ 스키마 → 모델: 일부 필드 누락

### 5.2 KPI 평가 기능
- ✅ 화면 → API 호출: 정상
- ✅ API → 스키마: 정상
- ✅ 스키마 → 모델: 정상
- ⚠️ 로직 구현: 일부 하드코딩 및 누락

## 6. 우선순위별 보완 작업

### 우선순위 1 (필수) - ✅ 완료
1. ✅ 스코어카드 생성 시 KPI 점수 포함
2. ✅ 평가 완료 후 스코어카드 자동 생성
3. ✅ 평가 태스크 구현

### 우선순위 2 (중요) - ✅ 완료
4. ✅ DataCollectionLog 스키마 보완
5. ✅ SNS 주목도 및 미디어 언급 빈도 실제 데이터 수집

## 7. 보완 작업 완료 내역

### 7.1 DataCollectionLog 스키마 보완 ✅

**파일**: `apps/api/app/schemas/data_collection.py`

**추가된 필드**:
- `prompt_template_id`: Optional[str]
- `perplexity_request`: Optional[Dict[str, Any]]
- `perplexity_response`: Optional[Dict[str, Any]]
- `token_usage`: Optional[Dict[str, Any]]

**효과**: 로그 상세 정보가 API 응답에 포함됨

### 7.2 SNS 주목도 및 미디어 언급 빈도 실제 데이터 수집 ✅

**파일**: `apps/api/app/services/ai_agents/evaluation_agent.py`

**수정 내용**:
- `_calculate_sns_attention_score()`: DataCollectionLog에서 sns 타입 로그 조회하여 점수 계산
- `_calculate_media_frequency_score()`: DataCollectionLog에서 media 타입 로그 조회하여 점수 계산

**효과**: 실제 수집된 데이터 기반으로 점수 계산

### 7.3 스코어카드 생성 시 KPI 점수 포함 ✅

**파일**: `apps/api/app/services/evaluation_service.py`

**수정 내용**:
- `complete_evaluation()`에서 `EvaluationScore`를 조회하여 7개 KPI 점수를 모두 포함
- 상위 3개 점수(ai_quantitative_score, sns_market_score, expert_survey_score)도 하위 호환성을 위해 포함

**효과**: 스코어카드에 모든 KPI 점수가 포함됨

### 7.4 평가 완료 후 스코어카드 자동 생성 ✅

**파일**: `apps/api/app/services/ai_agents/evaluation_agent.py`

**수정 내용**:
- `evaluate_async()` 완료 후 `EvaluationService.complete_evaluation()` 호출 추가
- 스코어카드 생성 실패해도 평가는 완료된 것으로 처리 (에러 로깅)

**효과**: 평가 완료 시 자동으로 스코어카드 생성됨

### 7.5 평가 태스크 구현 ✅

**파일**: `apps/api/app/tasks/evaluation_tasks.py`

**수정 내용**:
- `evaluate_report_task`에서 실제로 `evaluate_async()` 실행
- 비동기 함수 실행을 위한 이벤트 루프 생성
- 에러 발생 시 평가 상태를 실패로 업데이트

**효과**: Celery를 통한 비동기 평가가 정상 작동

### 7.6 SNS·시장 반응 점수 계산 개선 ✅

**파일**: `apps/api/app/services/evaluation_service.py`

**수정 내용**:
- `_calculate_sns_market_score()`에서 실제 DataCollectionLog 데이터 기반 계산
- SNS 주목도와 미디어 언급 빈도를 실제 수집된 데이터로 계산

**효과**: 실제 데이터 기반 점수 계산

### 7.7 프론트엔드 타입 안정성 개선 ✅

**파일**: `apps/web/src/app/data-collection/page.tsx`

**수정 내용**:
- `overall_progress` 타입을 `number | string`으로 변경하여 문자열/숫자 모두 처리

**효과**: API 응답의 타입 불일치 문제 해결

## 8. 최종 정합성 점검 결과

### 8.1 자료수집 기능
- ✅ 화면 → API 호출: 정상
- ✅ API → 스키마: 정상 (보완 완료)
- ✅ 스키마 → 모델: 정상 (보완 완료)
- ✅ 데이터 흐름: 정상

### 8.2 KPI 평가 기능
- ✅ 화면 → API 호출: 정상
- ✅ API → 스키마: 정상
- ✅ 스키마 → 모델: 정상
- ✅ 로직 구현: 정상 (보완 완료)
- ✅ 스코어카드 자동 생성: 정상 (보완 완료)

## 9. 개선 효과

### 9.1 자료수집 기능
- 로그 상세 정보 제공으로 디버깅 용이성 향상
- 스키마와 모델 일치로 데이터 무결성 확보

### 9.2 KPI 평가 기능
- 실제 데이터 기반 점수 계산으로 정확도 향상
- 스코어카드 자동 생성으로 작업 효율성 향상
- 모든 KPI 점수 포함으로 상세 분석 가능

## 10. 추가 권장 사항

### 10.1 데이터 수집 품질 개선
- Perplexity 응답 파싱 로직 강화
- SNS/미디어 데이터 구조 표준화
- 에러 처리 및 재시도 로직 추가

### 10.2 KPI 점수 계산 개선
- 점수 계산 기준값 조정 (데이터 축적 후)
- 가중치 조정 기능 추가
- 점수 히스토리 추적

### 10.3 모니터링 및 알림
- 평가 완료 알림 기능
- 데이터 수집 실패 알림
- 점수 변동 알림

